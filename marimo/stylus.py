import marimo

__generated_with = "0.15.3"
app = marimo.App(width="full")


@app.cell
def setup_pyoso():
    # This code sets up pyoso to be used as a database provider for this notebook
    # This code is autogenerated. Modification could lead to unexpected results :)
    import marimo as mo
    from pyoso import Client
    client = Client()
    try:
        pyoso_db_conn = client.dbapi_connection()    
    except Exception as e:
        pyoso_db_conn = None
    return client, mo


@app.cell
def about_app(mo):
    mo.vstack([
        mo.md("""
        # Stylus SDK Developer Ecosystem
        <small>Author: <span style="background-color: #f0f0f0; padding: 2px 4px; border-radius: 3px;">OSO Team</span> Â· Last Updated: <span style="background-color: #f0f0f0; padding: 2px 4px; border-radius: 3px;">2025-09-25</span></small>
        """),
        mo.md("""
        This dashboard analyzes the growth and impact of the Stylus SDK ecosystem on Arbitrum. It tracks developer engagement, project metrics, and ecosystem expansion through comprehensive data visualization and analysis.
        """),
        mo.accordion({
            "<b>Click to see details on how app was made</b>": mo.accordion({
                "Methodology": """
                - Analyzes developer engagement through fork events and commit activity
                - Uses PageRank algorithm to rank developers by influence and activity
                - Tracks project metrics including active developers, commits, and repository activity
                - Categorizes projects by ecosystem alignment (Stylus, Arbitrum, Solana, EVM)
                - Measures SDK adoption through dependency analysis
                """,
                "Data Sources": """
                - [OSO API](https://docs.opensource.observer/docs/get-started/python) - GitHub metrics and developer activity data
                - [Arbitrum Stylus Collection](https://docs.opensource.observer/docs/get-started/python) - Project and artifact data
                - [Package Dependencies](https://docs.opensource.observer/docs/get-started/python) - SDK usage and adoption metrics
                """,
                "Further Resources": """
                - [Getting Started with Pyoso](https://docs.opensource.observer/docs/get-started/python)
                - [Using the Semantic Layer](https://docs.opensource.observer/docs/get-started/using-semantic-layer)
                - [Marimo Documentation](https://docs.marimo.io/)
                """
            })
        })    
    ])
    return


@app.cell
def display_tabs(
    df_projects_from_devs,
    ecosystem_tab,
    mo,
    sdk_tab,
    stylus_sprint_tab,
    summary_tab,
):
    def _temp_tab(df, header):
        return mo.vstack([
            mo.md(f"## {header}"),
            mo.ui.table(
                data=df.reset_index(drop=True),
                selection=None,
                show_column_summaries=False,
                show_data_types=False,
                page_size=50
            )
        ])

    mo.ui.tabs({
        "Summary": summary_tab,
        "Ecosystem View": ecosystem_tab,
        "Stylus Sprint": stylus_sprint_tab,
        "SDK Usage": sdk_tab,
        "Developer Funnel": _temp_tab(df_projects_from_devs, "Projects by Developer Activity"),
    }, value="Ecosystem View")
    return


@app.cell
def _(
    df_stylus_project_metrics,
    df_stylus_sdk_deps,
    df_stylus_sdk_deps_project_metrics,
    make_joyplot,
    mo,
    summarize_monthly_metrics,
):
    _df = df_stylus_sdk_deps_project_metrics[df_stylus_sdk_deps_project_metrics['metric_name'] == 'GITHUB_project_velocity_daily']
    _mm = summarize_monthly_metrics(df_stylus_sdk_deps_project_metrics, num_months=6)

    _df_dep_repos = df_stylus_sdk_deps.copy()
    _df_dep_repos['Project Name'] = _df_dep_repos['dependent_project_name'].map(
        _df
        .drop_duplicates(subset=['display_name', 'project_name'])
        .set_index('project_name')['display_name']
        .to_dict()       
    )
    _df_dep_repos['In Stylus Sprint?'] = _df_dep_repos['dependent_project_name'].isin(df_stylus_project_metrics['project_name'])
    _df_dep_repos['Dependent Repo URL'] = _df_dep_repos.apply(lambda x: f"https://github.com/{x['repo_owner']}/{x['repo_name']}", axis=1)
    _ossd_base_url = 'https://github.com/opensource-observer/oss-directory/tree/main/data/projects'
    _df_dep_repos['OSS Directory URL'] = _df_dep_repos['dependent_project_name'].apply(lambda x: f"{_ossd_base_url}/{x[0]}/{x}.yaml")
    _df_dep_repos.drop(columns=['dependent_project_name', 'repo_owner', 'repo_name', 'artifact_id'], inplace=True)

    sdk_tab = mo.vstack([

        mo.md("### Developer Velocity"),
        mo.ui.plotly(
            figure=make_joyplot(df=_df, smoothing=7)
        ),

        mo.md("### Activities Over Last 6 Months"),
        mo.ui.table(
            data=_mm,
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=25,
            format_mapping={c: '{:,.0f}' for c in _mm.columns if c != 'Project'},
            freeze_columns_left=['Project']
        ),

        mo.md("### Registry of (Known) Projects"),
        mo.ui.table(
            data=_df_dep_repos.reset_index(drop=True),
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=50
        )

    ])   
    return (sdk_tab,)


@app.cell
def create_stylus_sprint_tab(
    df_stylus_project_metrics,
    df_stylus_sdk_deps_project_metrics,
    make_joyplot,
    mo,
    summarize_monthly_metrics,
):
    _df = df_stylus_project_metrics[df_stylus_project_metrics['metric_name'] == 'GITHUB_project_velocity_daily']
    _mm = summarize_monthly_metrics(df_stylus_project_metrics, num_months=6)

    _df_projects = _df[['display_name', 'project_name']].drop_duplicates()
    _df_projects['Using Stylus SDK?'] = _df_projects['project_name'].apply(lambda x: x in df_stylus_sdk_deps_project_metrics['project_name'].unique())

    _ossd_base_url = 'https://github.com/opensource-observer/oss-directory/tree/main/data/projects'
    _df_projects['OSS Directory URL'] = _df_projects['project_name'].apply(lambda x: f"{_ossd_base_url}/{x[0]}/{x}.yaml")
    _df_projects.rename(columns={'display_name': 'Project Name'}, inplace=True)
    _df_projects.drop(columns=['project_name'], inplace=True)
    _df_projects.sort_values(by='Project Name', inplace=True)

    stylus_sprint_tab = mo.vstack([

        mo.md("### Developer Velocity"),
        mo.ui.plotly(
            figure=make_joyplot(df=_df, smoothing=7)
        ),

        mo.md("### Activities Over Last 6 Months"),
        mo.ui.table(
            data=_mm,
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=20,
            format_mapping={c: '{:,.0f}' for c in _mm.columns if c != 'Project'},
            freeze_columns_left=['Project']
        ),

        mo.md("### Registry of (Known) Projects"),
        mo.ui.table(
            data=_df_projects.reset_index(drop=True),
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=50
        )
    ])   
    return (stylus_sprint_tab,)


@app.cell
def create_ecosystem_tab(
    df_ecosystem_metrics,
    df_repo_overlap,
    make_joyplot,
    make_repo_overlap_barchart,
    mo,
    summarize_monthly_metrics,
):
    _df = df_ecosystem_metrics[df_ecosystem_metrics['metric_name'] == 'GITHUB_project_velocity_daily']
    _mm = summarize_monthly_metrics(df_ecosystem_metrics, num_months=6)

    ecosystem_tab = mo.vstack([

        mo.md("### Developer Velocity"),
        mo.ui.plotly(
            figure=make_joyplot(df=_df, smoothing=7)
        ),

        mo.md("### Activities Over Last 6 Months"),
        mo.ui.table(
            data=_mm,
            selection=None,
            show_column_summaries=False,
            show_data_types=False,
            page_size=20,
            format_mapping={c: '{:,.0f}' for c in _mm.columns if c != 'Project'},
            freeze_columns_left=['Project']
        ),

        mo.md("### Repository Overlap"),
        mo.ui.plotly(
            figure=make_repo_overlap_barchart(df=df_repo_overlap)
        ),

    ])   
    return (ecosystem_tab,)


@app.cell
def create_summary_tab(
    df_fork_devs_ranked,
    df_projects_from_devs,
    df_stylus_project_metrics,
    df_stylus_sdk_deps,
    mo,
    most_recent_month,
):
    summary_tab = mo.vstack([
        mo.md("## Key Metrics"),
        mo.hstack(
            items=[
                mo.stat(
                    label="Stylus Sprint Projects",
                    bordered=True,
                    value=f"{len(df_stylus_project_metrics['project_name'].unique()):,.0f}",
                    caption="Projects with one or more open source repos"
                ),
                mo.stat(
                    label="SDK Dependents",
                    bordered=True,
                    value=f"{len(df_stylus_sdk_deps):,.0f}",
                    caption="Total repos that import the Stylus SDK"
                ),
                mo.stat(
                    label="Downstream Developers",
                    bordered=True,
                    value=f"{len(df_fork_devs_ranked):,.0f}",
                    caption="Developers using the SDK or who forked one of the examples"
                ),
                mo.stat(
                    label="Developers in Arbitrum Ecosystem",
                    bordered=True,
                    value=f"{len(df_projects_from_devs):,.0f}",
                    caption=f"Total active developers in {most_recent_month.strftime('%B %Y')} (c/o Electric Capital)"
                )
            ],
            widths="equal",
            gap=1
        )
    ])
    return (summary_tab,)


@app.cell
def generate_joyplot(SPRINT_START_DATE, go, np, pd, px):
    def _rgba_from_rgb(rgb, alpha):
        return rgb.replace("rgb", "rgba").replace(")", f",{alpha})")


    def make_joyplot(df, smoothing=7):

        colorscale = 'Greens'
        gap = 1.10
        fill_alpha = 0.40

        wide = df.pivot_table(index="date", columns="display_name", values="amount", aggfunc="sum").sort_index()
        if smoothing > 1:
            wide = wide.rolling(window=smoothing, min_periods=1, center=True).mean()

        wide_norm = wide.copy()
        for col in wide.columns:
            non_zero_count = (wide[col] > 0).sum()
            if non_zero_count > 50:
                threshold = int(wide[col].quantile(0.95))
                wide_norm[col] = wide_norm[col].clip(upper=threshold)
            denom = wide_norm[col].max(skipna=True)
            wide_norm[col] = wide_norm[col].divide(denom, axis=0)

        cols = list(wide_norm.columns)[::-1]
        cmap = getattr(px.colors.sequential, colorscale)
        cmap_subset = cmap[len(cmap)//2:][::-1]

        n = len(cols)
        proj_colors = {}
        for rank, col in enumerate(cols):
            idx = int(round(rank * (len(cmap_subset) - 1) / max(1, n - 1)))
            proj_colors[col] = cmap_subset[idx]

        sprint_start_date = pd.to_datetime(SPRINT_START_DATE).date()
        last_date = wide_norm.index.max()

        fig = go.Figure()
        tickvals, ticktext = [], []

        for i, col in enumerate(cols):
            y_offset = i * gap
            y_vals = wide_norm[col].fillna(0).values
            x_vals = wide_norm.index

            color = proj_colors[col]
            fill_color = _rgba_from_rgb(color, fill_alpha)

            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=np.full(y_vals.shape, y_offset, dtype=float),
                    mode="lines",
                    line=dict(width=0),
                    hoverinfo="skip",
                    showlegend=False
                )
            )
            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=y_vals + y_offset,
                    mode="lines",
                    fill="tonexty",
                    line=dict(color=color, width=1.5),
                    line_shape="spline",
                    fillcolor=fill_color,
                    name=col,
                    customdata=np.c_[wide_norm[col].reindex(x_vals).fillna(0).values * 100],
                    hovertemplate="<b>%{fullData.name}</b><br>Week of: %{x|%d %b %Y}<br>Velocity: %{customdata[0]:,.0f}% of max<extra></extra>",
                    showlegend=False
                )
            )
            tickvals.append(y_offset)

            # Calculate velocity change
            pre_sprint_data = wide_norm[col][wide_norm.index < sprint_start_date]
            post_sprint_data = wide_norm[col][wide_norm.index >= sprint_start_date]

            pre_sprint_avg = pre_sprint_data.fillna(0).mean()
            post_sprint_avg = post_sprint_data.fillna(0).mean()

            if pre_sprint_avg > 0:
                change = ((post_sprint_avg - pre_sprint_avg) / pre_sprint_avg) * 100
            else:
                change = 0

            if int(change) > 0:
                annotation_text = f"+{change:,.0f}%"
            elif int(change) < 0:
                annotation_text = f"{change:,.0f}%"
            else:
                annotation_text = "No change"

            ticktext.append(f"<b>{col}</b>: {annotation_text}")

        fig.add_vrect(
            x0=sprint_start_date, x1=last_date,
            fillcolor="rgba(128,128,128,0.05)",
            line_width=0.25,
            annotation_text="<i>Stylus Sprint</i>",
            annotation_position="top left",
        )

        fig.update_layout(
            template="plotly_white",
            paper_bgcolor="white",
            plot_bgcolor="white",
            font=dict(size=12, color="#111"),
            margin=dict(l=0, r=80, t=0, b=0),
            showlegend=False,
            title=dict(text="", x=0, xanchor="left", font=dict(size=14, color="#111"))
        )
        fig.update_xaxes(title="", showgrid=False, visible=False, linecolor="#000", linewidth=1)
        fig.update_yaxes(
            title="",
            side="right",
            showgrid=False,
            tickmode="array",
            ticklabelposition="outside top",
            #ticklabelshift=-1,
            ticklabelstandoff=5,
            tickvals=tickvals,
            ticktext=ticktext,
            zeroline=False,
            tickcolor="#000",
            showline=False
        )
        return fig
    return (make_joyplot,)


@app.cell
def generate_repo_overlap_barchart(go, np, pd):
    def make_repo_overlap_barchart(
        df: pd.DataFrame,
        target_ecosystem: str = "Arbitrum"
    ):
        # Expect columns: ecosystem_name, repos_shared_with_target, repos_not_shared_with_target, total_repos
        d = df.copy()
        d.sort_values(["repos_shared_with_target", "total_repos", "ecosystem_name"], ascending=[False,False,True], inplace=True)

        d["shared_val"] = d["repos_shared_with_target"]
        d["not_shared_val"] = d["repos_not_shared_with_target"]

        xaxis_title = "Repositories"
        hover_suffix = ""
        hover_shared_val = d["shared_val"]
        hover_not_shared_val = d["not_shared_val"]

        # Colors (fixed, readable on black)
        COLOR_SHARED = "#2ecc71"
        COLOR_NOT_SHARED = "#888888"
        COLOR_HILITE = "Green"  # slightly brighter for the highlighted ecosystem
        shared_colors = [COLOR_HILITE if name==target_ecosystem else COLOR_SHARED for name in d["ecosystem_name"]]

        fig = go.Figure()
        fig.add_trace(go.Bar(
            y=d["ecosystem_name"],
            x=d["shared_val"],
            name=f"Shared with {target_ecosystem}",
            orientation="h",
            marker=dict(color=shared_colors),
            hovertemplate=(
                "<b>%{y}</b><br>"
                "Shared: %{customdata[0]:,.0f}"+hover_suffix+"<br>"
                "Not shared: %{customdata[1]:,.0f}"+hover_suffix+"<br>"
                "Total: %{customdata[2]:,.0f}<extra></extra>"
            ),
            customdata=np.stack([
                hover_shared_val,
                hover_not_shared_val,
                d["total_repos"]
            ], axis=-1)
        ))
        fig.add_trace(go.Bar(
            y=d["ecosystem_name"],
            x=d["not_shared_val"],
            name="Not shared",
            orientation="h",
            marker=dict(color=COLOR_NOT_SHARED),
            hovertemplate=(
                "<b>%{y}</b><br>"
                "Shared: %{customdata[1]:,.0f}"+hover_suffix+"<br>"
                "Not shared: %{customdata[0]:,.0f}"+hover_suffix+"<br>"
                "Total: %{customdata[2]:,.0f}<extra></extra>"
            ),
            customdata=np.stack([
                hover_not_shared_val,
                hover_shared_val,
                d["total_repos"]
            ], axis=-1)
        ))
        fig.update_layout(
            barmode="stack",
            title="",
            paper_bgcolor="white", plot_bgcolor="white",
            font=dict(size=12, color="black"),
            margin=dict(t=0, l=0, r=20, b=40),
            showlegend=True,
            legend=dict(orientation="h", y=1.02, x=1, xanchor='right')
        )
        fig.update_xaxes(title=xaxis_title, showgrid=True, gridcolor="#AAA", zeroline=False, color="black")
        fig.update_yaxes(title="", showgrid=False, color="black", automargin=True)

        return fig
    return (make_repo_overlap_barchart,)


@app.cell
def configuration_settings():
    # Configuration constants
    SPRINT_START_DATE = '2025-03-01'
    PROJECT_START_DATE = '2024-01-01'
    ECOSYSTEM_START_DATE = '2024-01-01' #'2020-01-01'
    COLLECTION_NAME = 'arb-stylus'
    SDK_PROJECT_MAINTAINERS = ['offchainlabs', 'arbitrumfoundation']
    EVM_ECOSYSTEMS = [
        'arbitrum',
        'base',
        'bnb_chain',
        'evm_toolkit',
        'ethereum',
        'ethereum_l2s',
        'ethereum_virtual_machine_stack',
        'foundry',
        'polygon',
        'solidity',
    ]
    OTHER_ECOSYSTEMS = [
        'bitcoin',
        'cosmos_network_stack',
        'solana',
        'sui'
    ]
    ECOSYSTEMS = EVM_ECOSYSTEMS + OTHER_ECOSYSTEMS
    MONTHLY_METRICS = [
        'GITHUB_active_developers_monthly',
        'GITHUB_full_time_developers_monthly',
        'GITHUB_commits_monthly',    
        'GITHUB_releases_monthly',
        'GITHUB_forks_monthly',
        'GITHUB_stars_monthly',
        'GITHUB_opened_pull_requests_monthly',    
        'GITHUB_merged_pull_requests_monthly',
        'GITHUB_opened_issues_monthly',
        'GITHUB_closed_issues_monthly',
        'GITHUB_project_velocity_monthly',
        'GITHUB_bot_activity_monthly',
    ]
    WEEKLY_METRICS = [
        'GITHUB_commits_weekly',    
        'GITHUB_forks_weekly',
        'GITHUB_stars_weekly',
        'GITHUB_opened_pull_requests_weekly',    
        'GITHUB_merged_pull_requests_weekly',
        'GITHUB_opened_issues_weekly',
        'GITHUB_closed_issues_weekly',
        'GITHUB_project_velocity_weekly',
        'GITHUB_contributors_weekly'
    ]
    DAILY_METRICS = [
        'GITHUB_project_velocity_daily'
    ]
    METRIC_NAMES = MONTHLY_METRICS + WEEKLY_METRICS + DAILY_METRICS
    return (
        COLLECTION_NAME,
        ECOSYSTEMS,
        ECOSYSTEM_START_DATE,
        EVM_ECOSYSTEMS,
        METRIC_NAMES,
        MONTHLY_METRICS,
        OTHER_ECOSYSTEMS,
        PROJECT_START_DATE,
        SDK_PROJECT_MAINTAINERS,
        SPRINT_START_DATE,
    )


@app.cell
def get_data(
    COLLECTION_NAME,
    ECOSYSTEMS,
    ECOSYSTEM_START_DATE,
    EVM_ECOSYSTEMS,
    METRIC_NAMES,
    OTHER_ECOSYSTEMS,
    PROJECT_START_DATE,
    SDK_PROJECT_MAINTAINERS,
    client,
    pd,
    wraps,
):
    # QUERY HELPERS
    def parse_dates(*cols):
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                df = func(*args, **kwargs)
                for col in cols:
                    if col in df.columns:
                        df[col] = pd.to_datetime(df[col]).dt.date
                return df
            return wrapper
        return decorator

    stringify = lambda arr: "'" + "','".join(arr) + "'"

    # QUERIES
    @parse_dates("date")
    def get_metrics_by_developer_ecosystem():
        return client.to_pandas(f"""
            SELECT
              sample_date AS date,
              metric_name,
              projects_v1.display_name,
              amount
            FROM timeseries_metrics_by_project_v0
            JOIN metrics_v0 USING metric_id
            JOIN projects_v1 USING project_id
            WHERE
              sample_date >= DATE '{ECOSYSTEM_START_DATE}'
              AND metric_name IN ({stringify(METRIC_NAMES)})
              AND project_source = 'CRYPTO_ECOSYSTEMS'
              AND project_namespace = 'eco'
              AND project_name IN ({stringify(ECOSYSTEMS)})          
            ORDER BY 1,2,3  
            """)

    @parse_dates("date")
    def get_metrics_by_stylus_project():
        return client.to_pandas(f"""
            SELECT DISTINCT
              p.display_name AS display_name,
              p.project_name AS project_name,
              m.metric_name AS metric_name,
              ts.sample_date AS date,
              ts.amount AS amount
            FROM timeseries_metrics_by_project_v0 ts        
            JOIN metrics_v0 m
              ON m.metric_id = ts.metric_id
            JOIN projects_v1 p
              ON p.project_id = ts.project_id
            JOIN projects_by_collection_v1 pc
              ON p.project_id = pc.project_id
            WHERE
              ts.sample_date >= DATE '{PROJECT_START_DATE}'
              AND m.metric_name IN ({stringify(METRIC_NAMES)})
              AND pc.collection_name = '{COLLECTION_NAME}'
              AND ts.amount IS NOT NULL
        """)

    @parse_dates("date")
    def get_metrics_by_stylus_sdk_dependent(repo_ids):
        return client.to_pandas(f"""
            WITH ossd_projects AS (
              SELECT DISTINCT project_id
              FROM artifacts_by_project_v1
              WHERE
                artifact_id IN ({stringify(repo_ids)})
                AND artifact_source = 'GITHUB'
                AND project_source = 'OSS_DIRECTORY'
                AND project_namespace = 'oso'
                AND project_name NOT IN ({stringify(SDK_PROJECT_MAINTAINERS)})
            )
            SELECT DISTINCT
              p.display_name AS display_name,
              p.project_name AS project_name,
              m.metric_name AS metric_name,
              ts.sample_date AS date,
              ts.amount AS amount
            FROM timeseries_metrics_by_project_v0 ts        
            JOIN metrics_v0 m
              ON m.metric_id = ts.metric_id
            JOIN projects_v1 p
              ON p.project_id = ts.project_id
            WHERE
              ts.sample_date >= DATE '{PROJECT_START_DATE}'
              AND m.metric_name IN ({stringify(METRIC_NAMES)})
              AND p.project_id IN (SELECT project_id FROM ossd_projects)
              AND ts.amount IS NOT NULL
        """)    

    @parse_dates("date")
    def get_metrics_by_stylus_repo():
        return client.to_pandas(f"""
            SELECT
              ap.artifact_name AS artifact_name,
              ap.project_name AS project_name,
              m.metric_name AS metric_name,
              ts.sample_date AS date,
              ts.amount AS amount
            FROM artifacts_by_project_v1 ap
            JOIN projects_by_collection_v1 pc
              ON ap.project_id = pc.project_id
            JOIN timeseries_metrics_by_artifact_v0 ts
              ON ts.artifact_id = ap.artifact_id
            JOIN metrics_v0 m
              ON m.metric_id = ts.metric_id
            WHERE
              pc.collection_name = '{COLLECTION_NAME}'
              AND ap.artifact_source = 'GITHUB'
              AND m.metric_name = 'GITHUB_active_developers_monthly'
              AND ts.sample_date >= DATE '{PROJECT_START_DATE}'
        """)

    def get_stylus_sdk_dependents():
        return client.to_pandas(f"""
            WITH stylus AS (
              SELECT DISTINCT
                package_artifact_id,
                package_owner_artifact_id
              FROM package_owners_v0
              WHERE
                package_owner_artifact_namespace = 'offchainlabs'
                AND package_owner_artifact_name = 'stylus-sdk-rs'
                AND package_artifact_name = 'stylus-sdk'
            )
            SELECT DISTINCT
              abp.project_name AS dependent_project_name,
              sboms_v0.dependent_artifact_namespace AS repo_owner,
              sboms_v0.dependent_artifact_name AS repo_name,
              sboms_v0.dependent_artifact_id AS artifact_id          
            FROM sboms_v0
            JOIN stylus
              ON sboms_v0.package_artifact_id = stylus.package_artifact_id
            JOIN artifacts_by_project_v1 AS abp
              ON
                sboms_v0.dependent_artifact_id = abp.artifact_id
                AND abp.project_source = 'OSS_DIRECTORY'
                AND abp.project_namespace = 'oso'
                AND abp.project_name NOT IN ({stringify(SDK_PROJECT_MAINTAINERS)})
            WHERE stylus.package_owner_artifact_id != sboms_v0.dependent_artifact_id
            ORDER BY 1,2,3
        """)    

    @parse_dates("first_fork")
    def get_devs_who_forked_examples():
        return client.to_pandas(f"""
            WITH stylus AS (
              SELECT DISTINCT package_artifact_id
              FROM package_owners_v0
              WHERE
                package_owner_artifact_namespace = 'offchainlabs'
                AND package_owner_artifact_name = 'stylus-sdk-rs'
                AND package_artifact_name = 'stylus-sdk'
            ),
            example_repos AS (
              SELECT DISTINCT dependent_artifact_id
              FROM sboms_v0
              JOIN stylus USING package_artifact_id
              JOIN artifacts_by_project_v1 AS abp
                ON dependent_artifact_id = abp.artifact_id
                AND abp.project_source = 'OSS_DIRECTORY'
                AND abp.project_namespace = 'oso'
              WHERE abp.project_name IN ({stringify(SDK_PROJECT_MAINTAINERS)})
            ),
            devs AS (
              SELECT
                artifact_id AS dev_id,
                artifact_url AS dev_url,
                MIN(time) AS first_fork,
                MIN_BY(to_artifact_id, time) AS forked_repo_id
              FROM int_first_of_event_from_artifact__github AS fe
              JOIN example_repos AS er
                ON fe.to_artifact_id = er.dependent_artifact_id
              JOIN int_github_users AS u
                ON fe.from_artifact_id = u.artifact_id
              WHERE
                event_type = 'FORKED'
                AND time >= DATE '{PROJECT_START_DATE}'
              GROUP BY 1,2
            ),
            dev_events AS (
              SELECT
                devs.dev_id,
                devs.first_fork,
                e.to_artifact_id AS repo_id,
                SUM(e.amount) AS num_commits
              FROM int_events_daily__github AS e
              JOIN devs ON e.from_artifact_id = devs.dev_id
              WHERE
                e.event_type = 'COMMIT_CODE'
                AND e.bucket_day >= devs.first_fork
              GROUP BY 1,2,3
            ),
            star_counts AS (
              SELECT
                de.repo_id,
                APPROX_DISTINCT(e.from_artifact_id) AS star_count
              FROM int_events_daily__github AS e
              JOIN dev_events AS de
                ON e.to_artifact_id = de.repo_id
              WHERE
                e.event_type = 'STARRED'
                AND e.bucket_day >= de.first_fork
                AND e.from_artifact_id != de.dev_id
              GROUP BY 1
            )
            SELECT DISTINCT
              devs.dev_url,
              fr.artifact_url AS first_fork_repo_url,
              a.artifact_url AS repo_url,
              devs.first_fork,
              de.num_commits,
              COALESCE(sc.star_count, 0) AS star_count,          
              a.artifact_namespace AS repo_owner,
              de.dev_id,
              de.repo_id          
            FROM dev_events AS de
            JOIN devs ON de.dev_id = devs.dev_id
            JOIN int_artifacts__github AS a
              ON de.repo_id = a.artifact_id
            LEFT JOIN star_counts AS sc
              ON a.artifact_id = sc.repo_id
            JOIN repositories_v0 AS fr
              ON devs.forked_repo_id = fr.artifact_id
        """)

    def get_repo_alignment_tags(list_of_repo_artifact_ids):
        df = client.to_pandas(f"""
            WITH projects AS (
              SELECT DISTINCT
                artifact_id,
                project_id,
                project_name,
                project_source
              FROM artifacts_by_project_v1
              WHERE
                artifact_id IN ({stringify(list_of_repo_artifact_ids)})
                AND project_source IN ('OSS_DIRECTORY', 'CRYPTO_ECOSYSTEMS')
                AND project_namespace IN ('oso', 'eco')
            ),
            stylus_collection AS (
              SELECT DISTINCT
                project_id,
                True AS in_stylus_collection
              FROM projects_by_collection_v1 AS pbc
              JOIN projects USING project_id
              WHERE pbc.collection_name = '{COLLECTION_NAME}'
            ),
            alignment AS (
              SELECT
                project_id,
                MAX(CASE WHEN project_name = 'arbitrum' THEN True ELSE False END) AS in_arbitrum,
                MAX(CASE WHEN project_name IN ({stringify(OTHER_ECOSYSTEMS)}) THEN True ELSE False END) AS in_nonevem_ecosystem,
                MAX(CASE WHEN project_name IN ({stringify(EVM_ECOSYSTEMS)}) THEN True ELSE False END) AS in_evm_ecosystem
              FROM projects
              WHERE project_source = 'CRYPTO_ECOSYSTEMS'
              GROUP BY 1
            )
            SELECT DISTINCT
              projects.artifact_id AS repo_id,
              stylus_collection.*,
              alignment.*
            FROM projects
            LEFT JOIN stylus_collection USING project_id
            LEFT JOIN alignment USING project_id
        """)
        df = df.fillna(False)
        return df

    def get_artifact_overlap(target_ecosystem='Arbitrum'):
        return client.to_pandas(f"""
            WITH base AS (
              SELECT DISTINCT
                p.display_name AS ecosystem_name,
                ap.artifact_id
              FROM artifacts_by_project_v1 ap
              JOIN projects_v1 p ON p.project_id=ap.project_id
              WHERE
                ap.artifact_source='GITHUB'
                AND ap.project_source='CRYPTO_ECOSYSTEMS'
                AND ap.project_namespace='eco'
                AND ap.project_name IN ({stringify(ECOSYSTEMS)})
            ),
            target_set AS (
              SELECT artifact_id FROM base WHERE ecosystem_name='{target_ecosystem}'
            ),
            totals AS (
              SELECT
                ecosystem_name,
                COUNT(DISTINCT artifact_id) AS total_repos
              FROM base
              GROUP BY ecosystem_name
            ),
            shared AS (
              SELECT
                ecosystem_name,
                COUNT(DISTINCT artifact_id) AS shared_with_target
              FROM base
              WHERE artifact_id IN (SELECT artifact_id FROM target_set)
              GROUP BY ecosystem_name
            )
            SELECT
              t.ecosystem_name,
              COALESCE(s.shared_with_target,0) AS repos_shared_with_target,
              (t.total_repos - COALESCE(s.shared_with_target,0)) AS repos_not_shared_with_target,
              t.total_repos AS total_repos
            FROM totals t
            LEFT JOIN shared s
              ON t.ecosystem_name = s.ecosystem_name
            ORDER BY
              repos_shared_with_target DESC,
              ecosystem_name
    """)
    return (
        get_artifact_overlap,
        get_devs_who_forked_examples,
        get_metrics_by_developer_ecosystem,
        get_metrics_by_stylus_project,
        get_metrics_by_stylus_sdk_dependent,
        get_repo_alignment_tags,
        get_stylus_sdk_dependents,
    )


@app.cell
def process_data(
    dev_pagerank,
    get_artifact_overlap,
    get_devs_who_forked_examples,
    get_metrics_by_developer_ecosystem,
    get_metrics_by_stylus_project,
    get_metrics_by_stylus_sdk_dependent,
    get_stylus_sdk_dependents,
    label_repos,
):
    # Get the data
    df_ecosystem_metrics = get_metrics_by_developer_ecosystem()
    df_repo_overlap = get_artifact_overlap()
    df_stylus_project_metrics = get_metrics_by_stylus_project()
    df_stylus_sdk_deps = get_stylus_sdk_dependents()
    df_stylus_sdk_deps_project_metrics = get_metrics_by_stylus_sdk_dependent(df_stylus_sdk_deps['artifact_id'].unique())
    df_fork_devs = get_devs_who_forked_examples()

    # Process developer data
    df_fork_devs_labeled = label_repos(df_fork_devs)
    df_fork_devs_ranked = dev_pagerank(df_fork_devs)

    # Create projects summary
    df_projects_from_devs = (
        df_fork_devs_labeled.merge(df_fork_devs_ranked[['dev_id', 'dev_score']], on='dev_id')
        .groupby(['repo_owner', 'project_type'], as_index=False)
        .agg(
            repo_rank=('dev_score', 'sum'),
            first_fork=('first_fork','min'),    
            num_devs=('dev_id', 'nunique'),
            dev_names=('dev_url', lambda x: ' | '.join(sorted(set(x.replace('https://github.com/',''))))),
            repos_worked_on_by_those_devs=('repo_url', lambda x: ' | '.join(sorted(set(x))))
        )
        .sort_values(by=['repo_rank', 'num_devs', 'first_fork'], ascending=[False, False, True])
        .reset_index(drop=True)
        .drop(columns=['repo_rank'])
    )

    # Helper variables
    most_recent_month = df_stylus_project_metrics[df_stylus_project_metrics['metric_name'].str.contains('monthly')==True]['date'].max()
    return (
        df_ecosystem_metrics,
        df_fork_devs_ranked,
        df_projects_from_devs,
        df_repo_overlap,
        df_stylus_project_metrics,
        df_stylus_sdk_deps,
        df_stylus_sdk_deps_project_metrics,
        most_recent_month,
    )


@app.cell
def helper_monthly_metrics(MONTHLY_METRICS, datetime, most_recent_month):
    def summarize_monthly_metrics(df, num_months=6):    
        start_month = datetime(2025, most_recent_month.month - (num_months-1), 1).date()
        table = (
            df[
                (df['metric_name'].str.endswith('monthly') == True) &
                (df['date'].between(start_month, most_recent_month)) &
                (df['amount'].notna())
            ]
            .pivot_table(index='display_name', columns='metric_name', values='amount', aggfunc='sum', fill_value=0)
            .map(lambda x: round(x/num_months,2))
            [MONTHLY_METRICS]
            .rename(columns={c:clean_metric_name(c) for c in MONTHLY_METRICS})
            .reset_index()
            .rename(columns={'display_name': 'Project'})
        )
        return table
    return (summarize_monthly_metrics,)


@app.cell
def helper_labeling_and_pagerank(get_repo_alignment_tags, np, nx, pd):
    def label_repos(df):

        _repos = df['repo_id'].unique()

        df_labeled = get_repo_alignment_tags(_repos)
        df_labeled = df_labeled.groupby('repo_id').max()
        df_labeled["tags"] = df_labeled.apply(lambda row: ";".join([col.replace('in_','') for col in df_labeled.columns if row[col]]), axis=1)
        df_merged = df.set_index('repo_id').join(df_labeled[["tags"]])

        def label_project(tags, dev_url, repo_url):
            if not isinstance(tags, str):
                tags = ''
            if 'stylus' in tags:
                return 'Project (Stylus Sprint)'
            if '/offchainlabs/' in repo_url:
                return 'Offchain Labs'
            if 'arbitrum' in tags:
                return 'Project (Arbitrum Ecosystem)'            
            if 'nonevm' in tags:
                if ';' in tags:
                    return 'Project (EVM + Non-EVM Ecosystems)'
                else:
                    return 'Project (Non-EVM Ecosystem)'
            if len(tags) > 1:
                return 'Project (EVM Ecosystem)'
            if dev_url in repo_url:
                return 'Personal'
            return 'Project (Other)'

        df_merged['project_type'] = df_merged.apply(lambda x: label_project(x['tags'], x['dev_url'], x['repo_url']), axis=1)
        df_merged.drop(columns="tags", inplace=True)

        return df_merged



    def dev_pagerank(
        df,
        alpha=0.85,
        max_iter=100,
        star_exp=1.0,        # how strongly repo stars matter
        commit_exp=1.0,      # how strongly commit volume matters
        star_floor=0.0       # add to stars to avoid zero-sinks; e.g., 1.0
    ):

        # Nodes
        devs = df[['dev_id','dev_url']].drop_duplicates().reset_index(drop=True)
        repos = df[['repo_url','star_count']].drop_duplicates().reset_index(drop=True)

        G = nx.DiGraph()
        for _, r in devs.iterrows():
            G.add_node(("dev", r.dev_id), url=r.dev_url, ntype="dev")
        for _, r in repos.iterrows():
            stars = float(r.star_count or 0.0)
            G.add_node(("repo", r.repo_url), stars=stars, ntype="repo")

        # Edges (commit-weighted devârepo, star-weighted repoâdev)
        for _, r in df.iterrows():
            d = ("dev", r.dev_id); rep = ("repo", r.repo_url)
            if d not in G or rep not in G: 
                continue

            c = max(0.0, float(r.num_commits or 0.0))
            s = max(0.0, float(r.star_count  or 0.0))
            w_dr = c**commit_exp
            w_rd = (s + star_floor)**star_exp

            if w_dr > 0: G.add_edge(d,   rep, weight=w_dr)
            if w_rd > 0: G.add_edge(rep, d,   weight=w_rd)

        # Personalization: devs by commits, repos by stars
        dev_commit_sum = df.groupby("dev_id")["num_commits"].sum().reindex(devs.dev_id).fillna(0)
        repo_stars = repos.set_index("repo_url")["star_count"].fillna(0)

        p = {}
        for n in G.nodes:
            ntype, key = n
            if ntype == "dev":
                p[n] = 1.0 + np.log1p(float(dev_commit_sum.get(key, 0.0)))
            else:
                p[n] = 1.0 + np.log1p(float(repo_stars.get(key, 0.0)) + star_floor)

        z = sum(p.values()) or 1.0
        p = {k:v/z for k,v in p.items()}

        pr = nx.pagerank(G, alpha=alpha, personalization=p, weight="weight", max_iter=max_iter)
        rows = []
        for (ntype, key), score in pr.items():
            if ntype != "dev": 
                continue
            rows.append({
                "dev_id": key,
                "dev_score": score,
                "dev_url": G.nodes[(ntype,key)]["url"]
            })
        dev_rank = pd.DataFrame(rows).sort_values("dev_score", ascending=False, kind="mergesort").reset_index(drop=True)

        # Some diagnostics
        dev_rank = dev_rank.merge(
            devs.assign(total_commits=dev_commit_sum.values), on=["dev_id","dev_url"], how="left"
        )
        repo_star_map = repos.set_index("repo_url")["star_count"].to_dict()
        exposure = (
            df.assign(w=lambda x: (x["num_commits"].clip(lower=0).astype(float)**commit_exp) 
             * (df["repo_url"].map(repo_star_map).fillna(0).astype(float)))
              .groupby("dev_id")["w"]
              .sum()
              .rename("star_exposure")
        )
        dev_rank = dev_rank.merge(exposure, on="dev_id", how="left").fillna({"star_exposure":0.0})

        return dev_rank    
    return dev_pagerank, label_repos


@app.function
def clean_metric_name(col):
    return col.replace('GITHUB_','').replace('_monthly','').replace('_',' ').title()


@app.cell
def import_libraries():
    from datetime import datetime, date, timedelta
    from functools import wraps
    import pandas as pd
    import plotly.graph_objects as go
    import plotly.express as px
    import networkx as nx
    import numpy as np
    return datetime, go, np, nx, pd, px, wraps


if __name__ == "__main__":
    app.run()
